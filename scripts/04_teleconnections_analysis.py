#!/usr/bin/env python3
# scripts/04_teleconnections_analysis.py
"""
Script principal pour l'analyse des t√©l√©connexions - VERSION CORRIG√âE
"""

import sys
import os
import argparse
from pathlib import Path
import pandas as pd
import numpy as np

# ============================================================================
# CONFIGURATION DES IMPORTS
# ============================================================================

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

try:
    from src.data.climate_indices_loader import ClimateIndicesLoader
    from src.analysis.teleconnections import TeleconnectionsAnalyzer
    from src.config.settings import create_output_directories
    print("‚úÖ Tous les modules import√©s avec succ√®s")
except ImportError as e:
    print(f"‚ùå Erreur d'import: {e}")
    print("V√©rifiez que tous les modules sont pr√©sents dans le dossier src/")
    sys.exit(1)

# ============================================================================
# CLASSE PRINCIPALE D'ANALYSE DES T√âL√âCONNEXIONS - VERSION CORRIG√âE
# ============================================================================

class TeleconnectionsAnalysisMain:
    """
    Classe principale pour l'analyse des t√©l√©connexions.
    """
    
    def __init__(self, max_lag: int = 12):
        self.max_lag = max_lag
        
        # CORRECTION: D√©finir tous les chemins n√©cessaires
        self.events_file = project_root / "data/processed/extreme_events_senegal_final.csv"
        self.indices_raw_path = project_root / "data/raw/climate_indices"  # AJOUT√â
        self.indices_processed_file = project_root / "data/processed/climate_indices_combined.csv"
        
        # Initialisation des objets
        self.indices_loader = None  # Sera initialis√© dans step_1
        self.teleconnections_analyzer = None  # Sera initialis√© dans step_3
            
    def step_1_prepare_climate_indices(self) -> bool:
        """
        √âtape 1: Pr√©paration des indices climatiques.
        
        Returns:
            bool: True si succ√®s, False sinon
        """
        print("\n" + "="*80)
        print("√âTAPE 1: PR√âPARATION DES INDICES CLIMATIQUES")
        print("="*80)
        
        try:
            # CORRECTION: V√©rifier que le dossier existe
            if not self.indices_raw_path.exists():
                print(f"‚ùå Dossier des indices climatiques non trouv√©: {self.indices_raw_path}")
                print("Cr√©ez le dossier et placez-y vos fichiers d'indices (IOD, Nino34, TNA)")
                return False
            
            # Initialisation du loader avec le bon chemin
            self.indices_loader = ClimateIndicesLoader(str(self.indices_raw_path))
            
            # Chargement des indices individuels
            indices_dict = self.indices_loader.load_all_indices()
            
            if not indices_dict:
                print("‚ùå √âchec du chargement des indices climatiques")
                print("V√©rifiez que vos fichiers d'indices sont dans le bon format :")
                print(f"   - {self.indices_raw_path}/IOD_index.xlsx")
                print(f"   - {self.indices_raw_path}/Nino34_index.csv")
                print(f"   - {self.indices_raw_path}/TNA_index.csv")
                return False
            
            # Cr√©ation du dataset combin√©
            combined_df = self.indices_loader.create_combined_dataset()
            
            if combined_df.empty:
                print("‚ùå √âchec de la cr√©ation du dataset combin√©")
                return False
            
            # Sauvegarde
            self.indices_loader.save_processed_data()
            
            print(f"‚úÖ Indices climatiques pr√©par√©s avec succ√®s")
            print(f"   P√©riode: {combined_df.index.min().strftime('%Y-%m')} √† {combined_df.index.max().strftime('%Y-%m')}")
            print(f"   Indices: {list(combined_df.columns)}")
            print(f"   Observations: {len(combined_df)} mois")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la pr√©paration des indices: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def step_2_verify_extreme_events(self) -> bool:
        """
        √âtape 2: V√©rification des √©v√©nements extr√™mes.
        """
        print("\n" + "="*80)
        print("√âTAPE 2: V√âRIFICATION DES √âV√âNEMENTS EXTR√äMES")
        print("="*80)
        
        try:
            if not self.events_file.exists():
                print(f"‚ùå Fichier des √©v√©nements non trouv√©: {self.events_file}")
                print("Lancez d'abord le script de d√©tection des √©v√©nements:")
                print("   python scripts/01_detection_extremes.py")
                return False
            
            # Chargement rapide pour v√©rification
            df_events = pd.read_csv(self.events_file, index_col=0, parse_dates=True)
            
            print(f"‚úÖ √âv√©nements extr√™mes v√©rifi√©s")
            print(f"   Nombre d'√©v√©nements: {len(df_events)}")
            print(f"   P√©riode: {df_events.index.min().strftime('%Y-%m-%d')} √† {df_events.index.max().strftime('%Y-%m-%d')}")
            print(f"   Colonnes disponibles: {list(df_events.columns)}")
            
            # V√©rification de la coh√©rence temporelle
            if self.indices_processed_file.exists():
                df_indices = pd.read_csv(self.indices_processed_file, index_col=0, parse_dates=True)
                
                # V√©rification de l'overlap temporel
                events_start = df_events.index.min()
                events_end = df_events.index.max()
                indices_start = df_indices.index.min()
                indices_end = df_indices.index.max()
                
                overlap_start = max(events_start, indices_start)
                overlap_end = min(events_end, indices_end)
                
                if overlap_start < overlap_end:
                    overlap_years = (overlap_end - overlap_start).days / 365.25
                    print(f"   P√©riode de chevauchement: {overlap_start.strftime('%Y-%m')} √† {overlap_end.strftime('%Y-%m')} ({overlap_years:.1f} ans)")
                else:
                    print("‚ö†Ô∏è  Aucun chevauchement temporel d√©tect√© entre √©v√©nements et indices")
                    return False
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la v√©rification des √©v√©nements: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def step_3_analyze_teleconnections(self) -> bool:
        """
        √âtape 3: Analyse des t√©l√©connexions.
        """
        print("\n" + "="*80)
        print("√âTAPE 3: ANALYSE DES T√âL√âCONNEXIONS")
        print("="*80)
        
        try:
            # Initialisation de l'analyseur
            self.teleconnections_analyzer = TeleconnectionsAnalyzer()
            
            # Lancement de l'analyse compl√®te
            results = self.teleconnections_analyzer.run_complete_analysis(
                events_file=str(self.events_file),
                indices_file=str(self.indices_processed_file)
            )
            
            if not results:
                print("‚ùå √âchec de l'analyse des t√©l√©connexions")
                return False
            
            print(f"‚úÖ Analyse des t√©l√©connexions termin√©e avec succ√®s")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'analyse des t√©l√©connexions: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def step_4_generate_ml_features(self) -> bool:
        """
        √âtape 4: G√©n√©ration des features ML - VERSION SIMPLIFI√âE.
        """
        print("\n" + "="*80)
        print("√âTAPE 4: G√âN√âRATION DES FEATURES MACHINE LEARNING")
        print("="*80)
        
        try:
            # CORRECTION: V√©rifier que l'indices_loader existe
            if self.indices_loader is None:
                print("‚ö†Ô∏è  Loader d'indices non initialis√©, initialisation...")
                self.indices_loader = ClimateIndicesLoader(str(self.indices_raw_path))
                self.indices_loader.load_all_indices()
                self.indices_loader.create_combined_dataset()
            
            # V√©rifier que combined_data existe
            if not hasattr(self.indices_loader, 'combined_data') or self.indices_loader.combined_data is None:
                print("üîÑ Rechargement des indices...")
                self.indices_loader.load_all_indices()
                self.indices_loader.create_combined_dataset()
            
            # Cr√©ation des features avec d√©calages
            lagged_features = self.indices_loader.create_lagged_features(max_lag=self.max_lag)
            
            if lagged_features.empty:
                print("‚ùå √âchec de la cr√©ation des features")
                return False
            
            print(f"‚úÖ Features ML g√©n√©r√©es avec succ√®s")
            print(f"   Nombre de features: {lagged_features.shape[1]}")
            print(f"   Nombre d'observations: {lagged_features.shape[0]}")
            print(f"   P√©riode: {lagged_features.index.min().strftime('%Y-%m')} √† {lagged_features.index.max().strftime('%Y-%m')}")
            
            # Sauvegarde des features
            output_dir = project_root / "data/processed"
            output_dir.mkdir(parents=True, exist_ok=True)
            
            features_file = output_dir / f"climate_features_lag{self.max_lag}.csv"
            lagged_features.to_csv(features_file)
            
            print(f"   üíæ Features sauvegard√©es: {features_file}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration des features: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def step_5_prepare_ml_dataset(self) -> bool:
        """
        √âtape 5: Pr√©paration du dataset final pour ML.
        """
        print("\n" + "="*80)
        print("√âTAPE 5: PR√âPARATION DATASET MACHINE LEARNING")
        print("="*80)
        
        try:
            # Chargement des √©v√©nements extr√™mes
            df_events = pd.read_csv(self.events_file, index_col=0, parse_dates=True)
            
            # Chargement des features climatiques
            features_file = project_root / "data/processed" / f"climate_features_lag{self.max_lag}.csv"
            
            if not features_file.exists():
                print(f"‚ùå Fichier des features non trouv√©: {features_file}")
                print("Ex√©cutez d'abord l'√©tape 4 (g√©n√©ration des features)")
                return False
            
            df_features = pd.read_csv(features_file, index_col=0, parse_dates=True)
            
            # Cr√©ation de la s√©rie mensuelle d'√©v√©nements
            monthly_events = df_events.groupby(pd.Grouper(freq='MS')).size()
            monthly_events = monthly_events.reindex(df_features.index, fill_value=0)
            
            # Variable cible binaire (pr√©sence d'√©v√©nements extr√™mes dans le mois)
            target_binary = (monthly_events > 0).astype(int)
            
            # Variable cible continue (nombre d'√©v√©nements par mois)
            target_count = monthly_events
            
            # Variable cible intensit√© (pr√©cipitation maximale du mois)
            monthly_max_precip = df_events.groupby(pd.Grouper(freq='MS'))['max_precip'].max()
            target_intensity = monthly_max_precip.reindex(df_features.index, fill_value=0)
            
            # Dataset ML final
            ml_dataset = pd.DataFrame(index=df_features.index)
            
            # Ajout des variables cibles
            ml_dataset['target_occurrence'] = target_binary
            ml_dataset['target_count'] = target_count
            ml_dataset['target_intensity'] = target_intensity
            
            # Ajout des features climatiques
            for col in df_features.columns:
                ml_dataset[f'feature_{col}'] = df_features[col]
            
            # Ajout de features temporelles
            ml_dataset['month'] = ml_dataset.index.month
            ml_dataset['season'] = ml_dataset['month'].apply(
                lambda x: 1 if x in [5, 6, 7, 8, 9, 10] else 0  # 1=pluies, 0=s√®che
            )
            
            # Suppression des lignes avec trop de valeurs manquantes
            ml_dataset_clean = ml_dataset.dropna(thresh=int(len(ml_dataset.columns) * 0.8))
            
            print(f"‚úÖ Dataset ML pr√©par√© avec succ√®s")
            print(f"   Dimensions: {ml_dataset_clean.shape}")
            print(f"   Variables cibles: 3 (occurrence, count, intensity)")
            print(f"   Features climatiques: {len([c for c in ml_dataset_clean.columns if c.startswith('feature_')])}")
            print(f"   Features temporelles: 2 (month, season)")
            print(f"   P√©riode: {ml_dataset_clean.index.min().strftime('%Y-%m')} √† {ml_dataset_clean.index.max().strftime('%Y-%m')}")
            
            # Statistiques des variables cibles
            print(f"\nüìä STATISTIQUES DES VARIABLES CIBLES:")
            print(f"   Occurrence:")
            print(f"     Mois avec √©v√©nements: {ml_dataset_clean['target_occurrence'].sum()}/{len(ml_dataset_clean)} ({ml_dataset_clean['target_occurrence'].mean()*100:.1f}%)")
            print(f"   Count:")
            print(f"     √âv√©nements/mois (moyenne): {ml_dataset_clean['target_count'].mean():.2f}")
            print(f"     √âv√©nements/mois (max): {ml_dataset_clean['target_count'].max()}")
            print(f"   Intensity:")
            print(f"     Intensit√© moyenne: {ml_dataset_clean['target_intensity'].mean():.2f} mm")
            print(f"     Intensit√© maximale: {ml_dataset_clean['target_intensity'].max():.2f} mm")
            
            # Sauvegarde
            output_file = project_root / "data/processed" / "ml_dataset_teleconnections.csv"
            ml_dataset_clean.to_csv(output_file)
            
            print(f"\nüíæ Dataset ML sauvegard√©: {output_file}")
            
            # Cr√©ation d'un fichier de m√©tadonn√©es
            metadata = {
                'creation_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
                'source_events': str(self.events_file),
                'source_indices': str(features_file),
                'n_observations': len(ml_dataset_clean),
                'n_features_climate': len([c for c in ml_dataset_clean.columns if c.startswith('feature_')]),
                'n_features_temporal': 2,
                'max_lag_months': self.max_lag,
                'period_start': ml_dataset_clean.index.min().strftime('%Y-%m'),
                'period_end': ml_dataset_clean.index.max().strftime('%Y-%m'),
                'target_variables': ['target_occurrence', 'target_count', 'target_intensity'],
                'ready_for_ml': True
            }
            
            metadata_file = project_root / "data/processed" / "ml_dataset_metadata.json"
            import json
            with open(metadata_file, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            print(f"üìã M√©tadonn√©es sauvegard√©es: {metadata_file}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la pr√©paration du dataset ML: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def run_complete_analysis(self) -> bool:
        """
        Lance l'analyse compl√®te des t√©l√©connexions.
        """
        print("üåä ANALYSE COMPL√àTE DES T√âL√âCONNEXIONS OC√âAN-ATMOSPH√àRE")
        print("=" * 80)
        print("Analyse des relations entre indices climatiques (IOD, Nino34, TNA)")
        print("et √©v√©nements de pr√©cipitations extr√™mes au S√©n√©gal")
        print("=" * 80)
        
        # Cr√©er les dossiers de sortie
        create_output_directories()
        
        try:
            # √âtape 1: Pr√©paration des indices climatiques
            if not self.step_1_prepare_climate_indices():
                return False
            
            # √âtape 2: V√©rification des √©v√©nements extr√™mes
            if not self.step_2_verify_extreme_events():
                return False
            
            # √âtape 3: Analyse des t√©l√©connexions
            if not self.step_3_analyze_teleconnections():
                return False
            
            # √âtape 4: G√©n√©ration des features ML
            if not self.step_4_generate_ml_features():
                return False
            
            # √âtape 5: Pr√©paration du dataset ML final
            if not self.step_5_prepare_ml_dataset():
                return False
            
            # R√©sum√© final
            self.print_final_summary()
            
            return True
            
        except Exception as e:
            print(f"\n‚ùå ERREUR DURANT L'ANALYSE: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def print_final_summary(self):
        """
        Affiche le r√©sum√© final de l'analyse.
        """
        print("\n" + "="*80)
        print("‚úÖ ANALYSE DES T√âL√âCONNEXIONS TERMIN√âE AVEC SUCC√àS")
        print("="*80)
        
        # V√©rification des fichiers g√©n√©r√©s
        output_files = [
            ("Indices climatiques", project_root / "data/processed/climate_indices_combined.csv"),
            ("Features ML", project_root / "data/processed" / f"climate_features_lag{self.max_lag}.csv"),
            ("Dataset ML", project_root / "data/processed/ml_dataset_teleconnections.csv"),
            ("M√©tadonn√©es", project_root / "data/processed/ml_dataset_metadata.json"),
            ("Rapport t√©l√©connexions", project_root / "outputs/reports/rapport_teleconnexions.txt")
        ]
        
        print(f"üìä FICHIERS G√âN√âR√âS:")
        for name, filepath in output_files:
            if filepath.exists():
                print(f"   ‚úÖ {name}: {filepath.name}")
            else:
                print(f"   ‚ùå {name}: MANQUANT")
        
        # V√©rification des visualisations
        viz_dir = project_root / "outputs/visualizations/teleconnections"
        if viz_dir.exists():
            viz_files = list(viz_dir.glob("*.png"))
            print(f"\nüé® VISUALISATIONS CR√â√âES: {len(viz_files)} fichiers")
            for viz_file in viz_files:
                print(f"   üìà {viz_file.name}")
        
        print(f"\nüéØ PR√äT POUR LE MACHINE LEARNING:")
        print(f"   ‚Ä¢ Dataset structur√© avec variables cibles multiples")
        print(f"   ‚Ä¢ Features climatiques avec d√©calages optimaux")
        print(f"   ‚Ä¢ P√©riode d'entra√Ænement/test d√©finie")
        print(f"   ‚Ä¢ T√©l√©connexions quantifi√©es et document√©es")

# ============================================================================
# FONCTION PRINCIPALE
# ============================================================================

def main():
    """
    Fonction principale du script.
    """
    parser = argparse.ArgumentParser(
        description="Analyse des t√©l√©connexions oc√©an-atmosph√®re pour les pr√©cipitations extr√™mes au S√©n√©gal"
    )
    parser.add_argument(
        "--max-lag", 
        type=int, 
        default=12,
        help="D√©calage temporel maximum √† analyser en mois (d√©faut: 12)"
    )
    parser.add_argument(
        "--correlation-type",
        choices=['pearson', 'spearman'],
        default='pearson',
        help="Type de corr√©lation √† calculer (d√©faut: pearson)"
    )
    
    args = parser.parse_args()
    
    print("Script d'analyse des t√©l√©connexions oc√©an-atmosph√®re")
    print("Version int√©gr√©e avec architecture modulaire - CORRIG√âE")
    print("="*80)
    print(f"Configuration:")
    print(f"   D√©calage maximum: {args.max_lag} mois")
    print(f"   Type de corr√©lation: {args.correlation_type}")
    print("="*80)
    
    # Cr√©er et lancer l'analyseur
    analyzer = TeleconnectionsAnalysisMain(max_lag=args.max_lag)
    
    success = analyzer.run_complete_analysis()
    
    if success:
        print("\nüéâ ANALYSE R√âUSSIE!")
        print("Les t√©l√©connexions ont √©t√© quantifi√©es et le dataset ML est pr√™t.")
        print("Vous pouvez maintenant passer au d√©veloppement des mod√®les pr√©dictifs.")
        return 0
    else:
        print("\nüí• √âCHEC DE L'ANALYSE")
        print("V√©rifiez les erreurs ci-dessus et corrigez les probl√®mes.")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
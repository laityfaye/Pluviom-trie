#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# scripts/07_advanced_clustering_analysis.py
"""
Analyse de clustering avanc√©e avec comparaison de multiples algorithmes.
K-Means, DBSCAN, Clustering Hi√©rarchique, Gaussian Mixture Models, etc.

Auteur: Analyse Pr√©cipitations Extr√™mes
Date: 2025
"""

import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ML et clustering imports
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, adjusted_rand_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.neighbors import NearestNeighbors
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import pdist, squareform

# Configuration des chemins
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

class AdvancedClusteringAnalysis:
    """Analyse de clustering avanc√©e avec multiples algorithmes."""
    
    def __init__(self):
        """Initialise l'analyseur de clustering."""
        self.data = None
        self.features = None
        self.targets = None
        self.features_scaled = None
        self.scaler = StandardScaler()
        
        # R√©sultats des diff√©rents algorithmes
        self.clustering_results = {}
        self.evaluation_metrics = {}
        
        # Dossiers de sortie
        self.output_dir = project_root / "outputs"
        self.viz_dir = self.output_dir / "visualizations" / "clustering"
        self.report_dir = self.output_dir / "reports"
        
        for directory in [self.viz_dir, self.report_dir]:
            directory.mkdir(parents=True, exist_ok=True)
    
    def load_and_prepare_data(self):
        """Charge et pr√©pare les donn√©es pour le clustering."""
        print("üîÑ CHARGEMENT ET PR√âPARATION DES DONN√âES")
        print("=" * 50)
        
        try:
            # Charger le dataset ML
            ml_file = project_root / "data/processed/ml_dataset_teleconnections.csv"
            if not ml_file.exists():
                raise FileNotFoundError(f"Dataset ML non trouv√©: {ml_file}")
            
            self.data = pd.read_csv(ml_file, index_col=0, parse_dates=True)
            print(f"‚úÖ Dataset charg√©: {self.data.shape}")
            
            # S√©parer features et targets
            target_cols = ['occurrence', 'count', 'intensity']
            feature_cols = [col for col in self.data.columns if col not in target_cols]
            
            self.features = self.data[feature_cols]
            self.targets = self.data[target_cols]
            
            # Pr√©traitement
            self.features = self.features.fillna(self.features.mean())
            
            # Encoder les variables cat√©gorielles si n√©cessaire
            if 'season' in self.features.columns:
                le = LabelEncoder()
                self.features['season'] = le.fit_transform(self.features['season'].astype(str))
            
            # Normalisation
            self.features_scaled = pd.DataFrame(
                self.scaler.fit_transform(self.features),
                columns=self.features.columns,
                index=self.features.index
            )
            
            print(f"‚úÖ Donn√©es pr√©par√©es:")
            print(f"   Features: {self.features_scaled.shape[1]} variables")
            print(f"   Observations: {len(self.features_scaled)}")
            print(f"   P√©riode: {self.data.index.min()} √† {self.data.index.max()}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement: {e}")
            return False
    
    def perform_kmeans_analysis(self):
        """Analyse K-Means avec optimisation du nombre de clusters."""
        print(f"\nüéØ ANALYSE K-MEANS APPROFONDIE")
        print("=" * 50)
        
        # M√©thode du coude et silhouette
        k_range = range(2, 16)
        inertias = []
        silhouette_scores = []
        calinski_scores = []
        
        print("üîÑ Optimisation du nombre de clusters...")
        
        for k in k_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(self.features_scaled)
            
            inertias.append(kmeans.inertia_)
            silhouette_scores.append(silhouette_score(self.features_scaled, labels))
            calinski_scores.append(calinski_harabasz_score(self.features_scaled, labels))
        
        # D√©terminer le nombre optimal
        # M√©thode du coude automatis√©e
        def find_elbow(inertias):
            """Trouve le coude automatiquement."""
            # Calculer les d√©riv√©es secondes
            diffs = np.diff(inertias)
            diffs2 = np.diff(diffs)
            # Le coude est l√† o√π la d√©riv√©e seconde est maximale
            return np.argmax(diffs2) + 2  # +2 car on commence √† k=2
        
        elbow_k = find_elbow(inertias)
        silhouette_k = k_range[np.argmax(silhouette_scores)]
        calinski_k = k_range[np.argmax(calinski_scores)]
        
        # Choisir le meilleur k (consensus ou silhouette)
        optimal_k = silhouette_k
        
        print(f"‚úÖ Optimisation termin√©e:")
        print(f"   M√©thode du coude: k={elbow_k}")
        print(f"   Meilleur silhouette: k={silhouette_k} (score: {max(silhouette_scores):.3f})")
        print(f"   Meilleur Calinski-Harabasz: k={calinski_k}")
        print(f"   K optimal choisi: {optimal_k}")
        
        # Entra√Æner le mod√®le final
        kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
        kmeans_labels = kmeans_final.fit_predict(self.features_scaled)
        
        self.clustering_results['KMeans'] = {
            'algorithm': kmeans_final,
            'labels': kmeans_labels,
            'n_clusters': optimal_k,
            'optimization_scores': {
                'k_range': list(k_range),
                'inertias': inertias,
                'silhouette_scores': silhouette_scores,
                'calinski_scores': calinski_scores
            }
        }
        
        return optimal_k
    
    def perform_dbscan_analysis(self):
        """Analyse DBSCAN avec optimisation des param√®tres."""
        print(f"\nüîç ANALYSE DBSCAN")
        print("=" * 30)
        
        # Optimiser eps avec k-distance
        print("üîÑ Optimisation du param√®tre eps...")
        
        # Calculer les distances k-NN pour diff√©rentes valeurs de k
        k_values = [3, 4, 5, 6]
        eps_candidates = []
        
        for k in k_values:
            nbrs = NearestNeighbors(n_neighbors=k+1).fit(self.features_scaled)
            distances, indices = nbrs.kneighbors(self.features_scaled)
            # Prendre la k-i√®me distance (exclure la distance √† soi-m√™me)
            k_distances = np.sort(distances[:, k])
            # Utiliser le 95e percentile comme candidat eps
            eps_candidate = np.percentile(k_distances, 95)
            eps_candidates.append(eps_candidate)
        
        # Tester diff√©rentes combinaisons eps/min_samples
        eps_range = eps_candidates
        min_samples_range = [3, 4, 5, 6, 8, 10]
        
        best_score = -1
        best_params = None
        dbscan_results = []
        
        for eps in eps_range:
            for min_samples in min_samples_range:
                dbscan = DBSCAN(eps=eps, min_samples=min_samples)
                labels = dbscan.fit_predict(self.features_scaled)
                
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                n_noise = list(labels).count(-1)
                
                if n_clusters > 1:  # Au moins 2 clusters
                    # Calculer silhouette sans les points de bruit
                    non_noise_mask = labels != -1
                    if np.sum(non_noise_mask) > 10:  # Assez de points
                        score = silhouette_score(
                            self.features_scaled[non_noise_mask], 
                            labels[non_noise_mask]
                        )
                        
                        dbscan_results.append({
                            'eps': eps,
                            'min_samples': min_samples,
                            'n_clusters': n_clusters,
                            'n_noise': n_noise,
                            'silhouette_score': score
                        })
                        
                        if score > best_score:
                            best_score = score
                            best_params = (eps, min_samples)
        
        if best_params:
            eps_opt, min_samples_opt = best_params
            print(f"‚úÖ Param√®tres optimaux: eps={eps_opt:.3f}, min_samples={min_samples_opt}")
            
            # Entra√Æner le mod√®le final
            dbscan_final = DBSCAN(eps=eps_opt, min_samples=min_samples_opt)
            dbscan_labels = dbscan_final.fit_predict(self.features_scaled)
            
            n_clusters_final = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
            n_noise_final = list(dbscan_labels).count(-1)
            
            print(f"   Clusters trouv√©s: {n_clusters_final}")
            print(f"   Points aberrants: {n_noise_final}")
            print(f"   Silhouette score: {best_score:.3f}")
            
            self.clustering_results['DBSCAN'] = {
                'algorithm': dbscan_final,
                'labels': dbscan_labels,
                'n_clusters': n_clusters_final,
                'n_noise': n_noise_final,
                'best_params': best_params,
                'optimization_results': dbscan_results
            }
        else:
            print("‚ùå Aucune configuration DBSCAN satisfaisante trouv√©e")
    
    def perform_hierarchical_clustering(self):
        """Analyse de clustering hi√©rarchique."""
        print(f"\nüå≥ CLUSTERING HI√âRARCHIQUE")
        print("=" * 35)
        
        # Tester diff√©rentes m√©thodes de linkage
        linkage_methods = ['ward', 'complete', 'average', 'single']
        best_score = -1
        best_method = None
        best_n_clusters = None
        
        hierarchical_results = {}
        
        for method in linkage_methods:
            print(f"üîÑ Test m√©thode: {method}")
            
            # Tester diff√©rents nombres de clusters
            scores = []
            n_clusters_range = range(2, 11)
            
            for n_clusters in n_clusters_range:
                try:
                    agg = AgglomerativeClustering(
                        n_clusters=n_clusters, 
                        linkage=method
                    )
                    
                    labels = agg.fit_predict(self.features_scaled)
                    score = silhouette_score(self.features_scaled, labels)
                    scores.append(score)
                    
                    if score > best_score:
                        best_score = score
                        best_method = method
                        best_n_clusters = n_clusters
                        
                except Exception as e:
                    scores.append(-1)  # Score invalide
                    continue
            
            hierarchical_results[method] = {
                'n_clusters_range': list(n_clusters_range),
                'silhouette_scores': scores,
                'best_n_clusters': n_clusters_range[np.argmax(scores)] if max(scores) > -1 else None,
                'best_score': max(scores) if max(scores) > -1 else -1
            }
        
        if best_method is not None:
            print(f"‚úÖ Meilleure configuration: {best_method} avec {best_n_clusters} clusters")
            print(f"   Silhouette score: {best_score:.3f}")
            
            # Entra√Æner le mod√®le final
            agg_final = AgglomerativeClustering(
                n_clusters=best_n_clusters, 
                linkage=best_method
            )
            agg_labels = agg_final.fit_predict(self.features_scaled)
            
            # Calculer le dendrogramme pour visualisation
            if best_method == 'ward':
                linkage_matrix = linkage(self.features_scaled, method=best_method)
            else:
                # Pour les autres m√©thodes, calculer la matrice de distance
                distance_matrix = pdist(self.features_scaled)
                linkage_matrix = linkage(distance_matrix, method=best_method)
            
            self.clustering_results['Hierarchical'] = {
                'algorithm': agg_final,
                'labels': agg_labels,
                'n_clusters': best_n_clusters,
                'linkage_method': best_method,
                'linkage_matrix': linkage_matrix,
                'optimization_results': hierarchical_results
            }
        else:
            print("‚ùå Aucune configuration hi√©rarchique satisfaisante trouv√©e")
    
    def perform_gaussian_mixture_analysis(self):
        """Analyse avec Gaussian Mixture Models."""
        print(f"\nüé≠ GAUSSIAN MIXTURE MODELS")
        print("=" * 35)
        
        # Tester diff√©rents nombres de composantes
        n_components_range = range(2, 11)
        covariance_types = ['full', 'tied', 'diag', 'spherical']
        
        best_bic = float('inf')
        best_aic = float('inf')
        best_params = None
        gmm_results = []
        
        print("üîÑ Optimisation des param√®tres...")
        
        for n_components in n_components_range:
            for cov_type in covariance_types:
                try:
                    gmm = GaussianMixture(
                        n_components=n_components,
                        covariance_type=cov_type,
                        random_state=42
                    )
                    gmm.fit(self.features_scaled)
                    
                    bic = gmm.bic(self.features_scaled)
                    aic = gmm.aic(self.features_scaled)
                    
                    labels = gmm.predict(self.features_scaled)
                    silhouette = silhouette_score(self.features_scaled, labels)
                    
                    gmm_results.append({
                        'n_components': n_components,
                        'covariance_type': cov_type,
                        'bic': bic,
                        'aic': aic,
                        'silhouette_score': silhouette
                    })
                    
                    # Utiliser BIC pour la s√©lection (plus bas = mieux)
                    if bic < best_bic:
                        best_bic = bic
                        best_params = (n_components, cov_type)
                        
                except Exception as e:
                    # Certaines configurations peuvent √©chouer
                    continue
        
        if best_params:
            n_comp_opt, cov_type_opt = best_params
            print(f"‚úÖ Param√®tres optimaux: {n_comp_opt} composantes, covariance {cov_type_opt}")
            print(f"   BIC: {best_bic:.2f}")
            
            # Entra√Æner le mod√®le final
            gmm_final = GaussianMixture(
                n_components=n_comp_opt,
                covariance_type=cov_type_opt,
                random_state=42
            )
            gmm_final.fit(self.features_scaled)
            gmm_labels = gmm_final.predict(self.features_scaled)
            
            self.clustering_results['GaussianMixture'] = {
                'algorithm': gmm_final,
                'labels': gmm_labels,
                'n_clusters': n_comp_opt,
                'covariance_type': cov_type_opt,
                'bic': best_bic,
                'optimization_results': gmm_results
            }
        else:
            print("‚ùå Aucune configuration GMM satisfaisante trouv√©e")
    
    def perform_spectral_clustering(self):
        """Analyse avec Spectral Clustering."""
        print(f"\nüåà SPECTRAL CLUSTERING")
        print("=" * 30)
        
        # Tester diff√©rents nombres de clusters et param√®tres
        n_clusters_range = range(2, 11)
        affinity_types = ['rbf', 'nearest_neighbors']
        
        best_score = -1
        best_params = None
        spectral_results = []
        
        print("üîÑ Optimisation des param√®tres...")
        
        for n_clusters in n_clusters_range:
            for affinity in affinity_types:
                try:
                    spectral = SpectralClustering(
                        n_clusters=n_clusters,
                        affinity=affinity,
                        random_state=42
                    )
                    labels = spectral.fit_predict(self.features_scaled)
                    
                    score = silhouette_score(self.features_scaled, labels)
                    
                    spectral_results.append({
                        'n_clusters': n_clusters,
                        'affinity': affinity,
                        'silhouette_score': score
                    })
                    
                    if score > best_score:
                        best_score = score
                        best_params = (n_clusters, affinity)
                        
                except Exception as e:
                    continue
        
        if best_params:
            n_clusters_opt, affinity_opt = best_params
            print(f"‚úÖ Param√®tres optimaux: {n_clusters_opt} clusters, affinit√© {affinity_opt}")
            print(f"   Silhouette score: {best_score:.3f}")
            
            # Entra√Æner le mod√®le final
            spectral_final = SpectralClustering(
                n_clusters=n_clusters_opt,
                affinity=affinity_opt,
                random_state=42
            )
            spectral_labels = spectral_final.fit_predict(self.features_scaled)
            
            self.clustering_results['Spectral'] = {
                'algorithm': spectral_final,
                'labels': spectral_labels,
                'n_clusters': n_clusters_opt,
                'affinity': affinity_opt,
                'optimization_results': spectral_results
            }
        else:
            print("‚ùå Aucune configuration Spectral satisfaisante trouv√©e")
    
    def evaluate_clustering_methods(self):
        """√âvalue et compare tous les m√©thodes de clustering."""
        print(f"\nüìä √âVALUATION COMPARATIVE DES M√âTHODES")
        print("=" * 50)
        
        evaluation_metrics = {}
        
        for method_name, result in self.clustering_results.items():
            labels = result['labels']
            
            # Exclure les points de bruit pour DBSCAN
            if method_name == 'DBSCAN':
                non_noise_mask = labels != -1
                if np.sum(non_noise_mask) < 10:
                    continue
                features_eval = self.features_scaled[non_noise_mask]
                labels_eval = labels[non_noise_mask]
            else:
                features_eval = self.features_scaled
                labels_eval = labels
            
            # Calculer les m√©triques
            if len(set(labels_eval)) > 1:
                silhouette = silhouette_score(features_eval, labels_eval)
                calinski = calinski_harabasz_score(features_eval, labels_eval)
                davies_bouldin = davies_bouldin_score(features_eval, labels_eval)
                
                # Relation avec les targets
                occurrence_by_cluster = {}
                intensity_by_cluster = {}
                
                # Cr√©er un masque pour les donn√©es √©valu√©es
                if method_name == 'DBSCAN':
                    eval_targets = self.targets[non_noise_mask]
                else:
                    eval_targets = self.targets
                
                for cluster_id in set(labels_eval):
                    cluster_mask = labels_eval == cluster_id
                    
                    # Taux d'occurrence par cluster
                    occurrence_rate = eval_targets.loc[cluster_mask, 'occurrence'].mean()
                    occurrence_by_cluster[cluster_id] = occurrence_rate
                    
                    # Intensit√© moyenne par cluster (pour les √©v√©nements)
                    event_mask = cluster_mask & (eval_targets['occurrence'] == 1)
                    if event_mask.sum() > 0:
                        intensity_mean = eval_targets.loc[event_mask, 'intensity'].mean()
                        intensity_by_cluster[cluster_id] = intensity_mean
                    else:
                        intensity_by_cluster[cluster_id] = 0
                
                # Variance des taux d'occurrence (mesure de s√©paration)
                occurrence_variance = np.var(list(occurrence_by_cluster.values()))
                
                evaluation_metrics[method_name] = {
                    'silhouette_score': silhouette,
                    'calinski_harabasz_score': calinski,
                    'davies_bouldin_score': davies_bouldin,
                    'n_clusters': len(set(labels_eval)),
                    'occurrence_variance': occurrence_variance,
                    'occurrence_by_cluster': occurrence_by_cluster,
                    'intensity_by_cluster': intensity_by_cluster
                }
                
                print(f"‚úÖ {method_name}:")
                print(f"   Clusters: {len(set(labels_eval))}")
                print(f"   Silhouette: {silhouette:.3f}")
                print(f"   Calinski-Harabasz: {calinski:.2f}")
                print(f"   Davies-Bouldin: {davies_bouldin:.3f}")
                print(f"   Variance occurrence: {occurrence_variance:.4f}")
                
                if method_name == 'DBSCAN':
                    n_noise = result['n_noise']
                    print(f"   Points aberrants: {n_noise}")
        
        self.evaluation_metrics = evaluation_metrics
        return evaluation_metrics
    
    def create_comprehensive_visualizations(self):
        """Cr√©e toutes les visualisations pour l'analyse de clustering."""
        print(f"\nüé® CR√âATION DES VISUALISATIONS")
        print("=" * 50)
        
        if not self.clustering_results:
            print("‚ùå Aucun r√©sultat de clustering disponible")
            return
        
        # 1. Comparaison PCA des diff√©rents algorithmes
        if len(self.clustering_results) > 0:
            # Calculer PCA une seule fois
            pca = PCA(n_components=2)
            features_pca = pca.fit_transform(self.features_scaled)
            
            n_algorithms = len(self.clustering_results)
            fig, axes = plt.subplots(1, n_algorithms, figsize=(6*n_algorithms, 6))
            if n_algorithms == 1:
                axes = [axes]
            
            fig.suptitle('Comparaison des Algorithmes de Clustering (PCA)', 
                        fontsize=16, fontweight='bold')
            
            for i, (method_name, result) in enumerate(self.clustering_results.items()):
                labels = result['labels']
                
                # Scatter plot avec PCA
                scatter = axes[i].scatter(features_pca[:, 0], features_pca[:, 1], 
                                        c=labels, cmap='viridis', alpha=0.7, s=30)
                
                # Points de bruit en noir pour DBSCAN
                if method_name == 'DBSCAN':
                    noise_mask = labels == -1
                    if noise_mask.sum() > 0:
                        axes[i].scatter(features_pca[noise_mask, 0], features_pca[noise_mask, 1], 
                                      c='black', s=20, alpha=0.5, label='Bruit')
                        axes[i].legend()
                
                title = f"{method_name}\n({result['n_clusters']} clusters"
                if method_name == 'DBSCAN' and 'n_noise' in result:
                    title += f", {result['n_noise']} bruit"
                title += ")"
                
                axes[i].set_title(title, fontweight='bold')
                axes[i].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
                axes[i].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
                axes[i].grid(True, alpha=0.3)
                
                # Colorbar pour chaque subplot
                plt.colorbar(scatter, ax=axes[i])
            
            plt.tight_layout()
            plt.savefig(self.viz_dir / "clustering_comparison_pca.png", dpi=300, bbox_inches='tight')
            plt.close()
        
        # 2. M√©triques comparatives
        if self.evaluation_metrics:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('M√©triques Comparatives des Algorithmes de Clustering', 
                        fontsize=16, fontweight='bold')
            
            methods = list(self.evaluation_metrics.keys())
            
            # Silhouette scores
            silhouette_scores = [self.evaluation_metrics[m]['silhouette_score'] for m in methods]
            bars1 = axes[0, 0].bar(methods, silhouette_scores, color=plt.cm.viridis(np.linspace(0, 1, len(methods))))
            axes[0, 0].set_title('Silhouette Score (plus haut = mieux)', fontweight='bold')
            axes[0, 0].set_ylabel('Score')
            axes[0, 0].tick_params(axis='x', rotation=45)
            
            for bar, score in zip(bars1, silhouette_scores):
                axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                               f'{score:.3f}', ha='center', va='bottom', fontweight='bold')
            
            # Calinski-Harabasz scores
            calinski_scores = [self.evaluation_metrics[m]['calinski_harabasz_score'] for m in methods]
            bars2 = axes[0, 1].bar(methods, calinski_scores, color=plt.cm.plasma(np.linspace(0, 1, len(methods))))
            axes[0, 1].set_title('Calinski-Harabasz Score (plus haut = mieux)', fontweight='bold')
            axes[0, 1].set_ylabel('Score')
            axes[0, 1].tick_params(axis='x', rotation=45)
            
            for bar, score in zip(bars2, calinski_scores):
                axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(calinski_scores)*0.01,
                               f'{score:.1f}', ha='center', va='bottom', fontweight='bold')
            
            # Davies-Bouldin scores
            davies_scores = [self.evaluation_metrics[m]['davies_bouldin_score'] for m in methods]
            bars3 = axes[1, 0].bar(methods, davies_scores, color=plt.cm.coolwarm(np.linspace(0, 1, len(methods))))
            axes[1, 0].set_title('Davies-Bouldin Score (plus bas = mieux)', fontweight='bold')
            axes[1, 0].set_ylabel('Score')
            axes[1, 0].tick_params(axis='x', rotation=45)
            
            for bar, score in zip(bars3, davies_scores):
                axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(davies_scores)*0.01,
                               f'{score:.3f}', ha='center', va='bottom', fontweight='bold')
            
            # Variance des taux d'occurrence
            occurrence_variances = [self.evaluation_metrics[m]['occurrence_variance'] for m in methods]
            bars4 = axes[1, 1].bar(methods, occurrence_variances, color=plt.cm.Spectral(np.linspace(0, 1, len(methods))))
            axes[1, 1].set_title('Variance des Taux d\'Occurrence\n(plus haut = meilleure s√©paration)', fontweight='bold')
            axes[1, 1].set_ylabel('Variance')
            axes[1, 1].tick_params(axis='x', rotation=45)
            
            for bar, var in zip(bars4, occurrence_variances):
                axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(occurrence_variances)*0.01,
                               f'{var:.4f}', ha='center', va='bottom', fontweight='bold')
            
            plt.tight_layout()
            plt.savefig(self.viz_dir / "clustering_metrics_comparison.png", dpi=300, bbox_inches='tight')
            plt.close()
        
        # 3. Optimisation K-Means d√©taill√©e
        if 'KMeans' in self.clustering_results:
            opt_scores = self.clustering_results['KMeans']['optimization_scores']
            
            fig, axes = plt.subplots(1, 3, figsize=(18, 6))
            fig.suptitle('Optimisation K-Means - M√©thodes de S√©lection du Nombre de Clusters', 
                        fontsize=16, fontweight='bold')
            
            k_range = opt_scores['k_range']
            
            # M√©thode du coude
            axes[0].plot(k_range, opt_scores['inertias'], 'bo-', linewidth=2, markersize=8)
            axes[0].set_title('M√©thode du Coude', fontweight='bold')
            axes[0].set_xlabel('Nombre de clusters (k)')
            axes[0].set_ylabel('Inertie')
            axes[0].grid(True, alpha=0.3)
            
            # Silhouette scores
            axes[1].plot(k_range, opt_scores['silhouette_scores'], 'ro-', linewidth=2, markersize=8)
            optimal_k = self.clustering_results['KMeans']['n_clusters']
            axes[1].axvline(x=optimal_k, color='red', linestyle='--', alpha=0.7, 
                           label=f'Optimal k={optimal_k}')
            axes[1].set_title('Silhouette Score', fontweight='bold')
            axes[1].set_xlabel('Nombre de clusters (k)')
            axes[1].set_ylabel('Silhouette Score')
            axes[1].legend()
            axes[1].grid(True, alpha=0.3)
            
            # Calinski-Harabasz scores
            axes[2].plot(k_range, opt_scores['calinski_scores'], 'go-', linewidth=2, markersize=8)
            axes[2].set_title('Calinski-Harabasz Score', fontweight='bold')
            axes[2].set_xlabel('Nombre de clusters (k)')
            axes[2].set_ylabel('Calinski-Harabasz Score')
            axes[2].grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(self.viz_dir / "kmeans_optimization.png", dpi=300, bbox_inches='tight')
            plt.close()
        
        # 4. Dendrogramme pour clustering hi√©rarchique
        if 'Hierarchical' in self.clustering_results:
            plt.figure(figsize=(15, 8))
            linkage_matrix = self.clustering_results['Hierarchical']['linkage_matrix']
            
            dendrogram(linkage_matrix, 
                      truncate_mode='level', 
                      p=10,  # Limiter le nombre de niveaux affich√©s
                      show_leaf_counts=True)
            
            plt.title('Dendrogramme - Clustering Hi√©rarchique', fontsize=16, fontweight='bold')
            plt.xlabel('Nombre d\'√©chantillons dans le n≈ìud')
            plt.ylabel('Distance')
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(self.viz_dir / "hierarchical_dendrogram.png", dpi=300, bbox_inches='tight')
            plt.close()
        
        print("‚úÖ Toutes les visualisations ont √©t√© cr√©√©es")
    
    def generate_comprehensive_report(self):
        """G√©n√®re un rapport complet de l'analyse de clustering."""
        print(f"\nüìÑ G√âN√âRATION DU RAPPORT COMPLET")
        print("=" * 50)
        
        report_path = self.report_dir / "rapport_clustering_avance.txt"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("RAPPORT COMPLET - ANALYSE DE CLUSTERING AVANC√âE\n")
            f.write("=" * 80 + "\n\n")
            
            f.write(f"Date de g√©n√©ration: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"P√©riode d'analyse: {self.data.index.min()} √† {self.data.index.max()}\n")
            f.write(f"Nombre d'observations: {len(self.features_scaled)}\n")
            f.write(f"Nombre de features: {self.features_scaled.shape[1]}\n\n")
            
            # Section algorithmes test√©s
            f.write("ALGORITHMES TEST√âS\n")
            f.write("-" * 25 + "\n")
            for method_name, result in self.clustering_results.items():
                f.write(f"‚Ä¢ {method_name}: {result['n_clusters']} clusters\n")
                if method_name == 'DBSCAN' and 'n_noise' in result:
                    f.write(f"  - Points aberrants d√©tect√©s: {result['n_noise']}\n")
            f.write("\n")
            
            # Section m√©triques comparatives
            if self.evaluation_metrics:
                f.write("M√âTRIQUES COMPARATIVES\n")
                f.write("-" * 30 + "\n")
                
                # Tableau comparatif
                f.write(f"{'Algorithme':<15} {'Clusters':<10} {'Silhouette':<12} {'Calinski-H':<12} {'Davies-B':<12} {'Var.Occur.':<12}\n")
                f.write("-" * 85 + "\n")
                
                for method_name, metrics in self.evaluation_metrics.items():
                    f.write(f"{method_name:<15} {metrics['n_clusters']:<10} "
                           f"{metrics['silhouette_score']:<12.3f} "
                           f"{metrics['calinski_harabasz_score']:<12.1f} "
                           f"{metrics['davies_bouldin_score']:<12.3f} "
                           f"{metrics['occurrence_variance']:<12.4f}\n")
                
                f.write("\n")
                
                # Meilleur algorithme selon diff√©rents crit√®res
                best_silhouette = max(self.evaluation_metrics.items(), 
                                    key=lambda x: x[1]['silhouette_score'])
                best_calinski = max(self.evaluation_metrics.items(), 
                                  key=lambda x: x[1]['calinski_harabasz_score'])
                best_davies = min(self.evaluation_metrics.items(), 
                                key=lambda x: x[1]['davies_bouldin_score'])
                best_occurrence = max(self.evaluation_metrics.items(), 
                                    key=lambda x: x[1]['occurrence_variance'])
                
                f.write("CLASSEMENT PAR CRIT√àRE\n")
                f.write("-" * 25 + "\n")
                f.write(f"Meilleur Silhouette: {best_silhouette[0]} ({best_silhouette[1]['silhouette_score']:.3f})\n")
                f.write(f"Meilleur Calinski-Harabasz: {best_calinski[0]} ({best_calinski[1]['calinski_harabasz_score']:.1f})\n")
                f.write(f"Meilleur Davies-Bouldin: {best_davies[0]} ({best_davies[1]['davies_bouldin_score']:.3f})\n")
                f.write(f"Meilleure s√©paration √©v√©nements: {best_occurrence[0]} ({best_occurrence[1]['occurrence_variance']:.4f})\n\n")
            
            # Section analyse par algorithme
            f.write("ANALYSE D√âTAILL√âE PAR ALGORITHME\n")
            f.write("-" * 40 + "\n\n")
            
            for method_name, result in self.clustering_results.items():
                f.write(f"{method_name.upper()}\n")
                f.write("=" * len(method_name) + "\n")
                
                f.write(f"Nombre de clusters: {result['n_clusters']}\n")
                
                if method_name == 'KMeans':
                    f.write(f"M√©thode d'optimisation: Silhouette + Coude + Calinski-Harabasz\n")
                elif method_name == 'DBSCAN':
                    if 'best_params' in result:
                        eps, min_samples = result['best_params']
                        f.write(f"Param√®tres optimaux: eps={eps:.3f}, min_samples={min_samples}\n")
                    if 'n_noise' in result:
                        f.write(f"Points aberrants: {result['n_noise']}\n")
                elif method_name == 'Hierarchical':
                    f.write(f"M√©thode de linkage: {result['linkage_method']}\n")
                elif method_name == 'GaussianMixture':
                    if 'covariance_type' in result:
                        f.write(f"Type de covariance: {result['covariance_type']}\n")
                        f.write(f"BIC: {result['bic']:.2f}\n")
                elif method_name == 'Spectral':
                    if 'affinity' in result:
                        f.write(f"Type d'affinit√©: {result['affinity']}\n")
                
                # Analyse des clusters par rapport aux √©v√©nements
                if method_name in self.evaluation_metrics:
                    metrics = self.evaluation_metrics[method_name]
                    f.write(f"\nRelation avec les √©v√©nements extr√™mes:\n")
                    
                    for cluster_id, occurrence_rate in metrics['occurrence_by_cluster'].items():
                        intensity = metrics['intensity_by_cluster'].get(cluster_id, 0)
                        f.write(f"  Cluster {cluster_id}: {occurrence_rate:.1%} d'occurrence, "
                               f"{intensity:.1f} mm d'intensit√© moyenne\n")
                
                f.write("\n" + "-" * 60 + "\n\n")
            
            # Recommandations
            f.write("RECOMMANDATIONS\n")
            f.write("-" * 20 + "\n")
            
            if self.evaluation_metrics:
                # Algorithme recommand√© bas√© sur un score composite
                composite_scores = {}
                for method_name, metrics in self.evaluation_metrics.items():
                    # Score composite (normalis√©)
                    silhouette_norm = metrics['silhouette_score']  # [0, 1]
                    occurrence_var_norm = min(1, metrics['occurrence_variance'] * 10)  # Ajuster √©chelle
                    
                    composite_score = (silhouette_norm + occurrence_var_norm) / 2
                    composite_scores[method_name] = composite_score
                
                best_overall = max(composite_scores.items(), key=lambda x: x[1])
                
                f.write(f"ALGORITHME RECOMMAND√â: {best_overall[0]}\n")
                f.write(f"Score composite: {best_overall[1]:.3f}\n\n")
                
                f.write("Justification:\n")
                best_metrics = self.evaluation_metrics[best_overall[0]]
                f.write(f"‚Ä¢ Bon √©quilibre entre qualit√© du clustering (Silhouette: {best_metrics['silhouette_score']:.3f})\n")
                f.write(f"‚Ä¢ Bonne s√©paration des conditions √† risque (Variance: {best_metrics['occurrence_variance']:.4f})\n")
                f.write(f"‚Ä¢ {best_metrics['n_clusters']} clusters identifi√©s\n\n")
            
            f.write("APPLICATIONS OP√âRATIONNELLES:\n")
            f.write("‚Ä¢ Classification automatique des conditions climatiques\n")
            f.write("‚Ä¢ Identification des r√©gimes m√©t√©orologiques √† risque\n")
            f.write("‚Ä¢ Am√©lioration des mod√®les de pr√©vision\n")
            f.write("‚Ä¢ Stratification des donn√©es pour l'apprentissage automatique\n")
            f.write("‚Ä¢ Analyse des t√©l√©connexions par type de conditions\n\n")
            
            f.write("PROCHAINES √âTAPES SUGG√âR√âES:\n")
            f.write("‚Ä¢ Validation temporelle des clusters\n")
            f.write("‚Ä¢ Analyse saisonni√®re du clustering\n")
            f.write("‚Ä¢ Int√©gration dans les mod√®les pr√©dictifs\n")
            f.write("‚Ä¢ Tests sur d'autres r√©gions d'Afrique de l'Ouest\n")
        
        print(f"‚úÖ Rapport complet g√©n√©r√©: {report_path}")
        return report_path
    
    def run_complete_clustering_analysis(self):
        """Ex√©cute l'analyse compl√®te de clustering."""
        print("üéØ ANALYSE DE CLUSTERING AVANC√âE")
        print("=" * 70)
        print("Comparaison de K-Means, DBSCAN, Hi√©rarchique, GMM et Spectral")
        print("=" * 70)
        
        start_time = datetime.now()
        
        # √âtape 1: Chargement des donn√©es
        if not self.load_and_prepare_data():
            return False
        
        # √âtape 2: K-Means (r√©f√©rence)
        optimal_k = self.perform_kmeans_analysis()
        
        # √âtape 3: DBSCAN
        self.perform_dbscan_analysis()
        
        # √âtape 4: Clustering hi√©rarchique
        self.perform_hierarchical_clustering()
        
        # √âtape 5: Gaussian Mixture Models
        self.perform_gaussian_mixture_analysis()
        
        # √âtape 6: Spectral Clustering
        self.perform_spectral_clustering()
        
        # √âtape 7: √âvaluation comparative
        evaluation_metrics = self.evaluate_clustering_methods()
        
        # √âtape 8: Visualisations
        self.create_comprehensive_visualizations()
        
        # √âtape 9: Rapport complet
        report_path = self.generate_comprehensive_report()
        
        # R√©sum√© final
        duration = (datetime.now() - start_time).total_seconds()
        
        print(f"\n" + "=" * 70)
        print("‚úÖ ANALYSE DE CLUSTERING AVANC√âE TERMIN√âE!")
        print("=" * 70)
        
        print(f"‚è±Ô∏è  Dur√©e totale: {duration:.1f} secondes")
        print(f"üéØ Algorithmes test√©s: {len(self.clustering_results)}")
        print(f"üìä M√©triques calcul√©es: {len(evaluation_metrics)}")
        
        if evaluation_metrics:
            # Meilleur algorithme
            best_method = max(evaluation_metrics.items(), 
                            key=lambda x: (x[1]['silhouette_score'] + min(1, x[1]['occurrence_variance']*10))/2)
            
            print(f"üèÜ Meilleur algorithme: {best_method[0]}")
            print(f"   Silhouette: {best_method[1]['silhouette_score']:.3f}")
            print(f"   Clusters: {best_method[1]['n_clusters']}")
            print(f"   S√©paration √©v√©nements: {best_method[1]['occurrence_variance']:.4f}")
        
        print(f"\nüìÅ FICHIERS G√âN√âR√âS:")
        viz_files = list(self.viz_dir.glob("*.png"))
        print(f"   üé® Visualisations: {len(viz_files)} fichiers")
        for viz_file in viz_files:
            print(f"      ‚Ä¢ {viz_file.name}")
        
        print(f"   üìÑ Rapport: {report_path}")
        
        print(f"\nüöÄ APPLICATIONS RECOMMAND√âES:")
        print(f"   ‚Ä¢ Utiliser le meilleur algorithme pour stratifier les donn√©es")
        print(f"   ‚Ä¢ Am√©liorer les mod√®les ML avec les labels de clusters")
        print(f"   ‚Ä¢ D√©velopper des alertes sp√©cifiques par cluster")
        print(f"   ‚Ä¢ Analyser les t√©l√©connexions par r√©gime climatique")
        
        return True

# ============================================================================
# FONCTION PRINCIPALE
# ============================================================================

def main():
    """Fonction principale."""
    print("üéØ ANALYSE DE CLUSTERING AVANC√âE - PR√âCIPITATIONS EXTR√äMES")
    print("=" * 70)
    
    analyzer = AdvancedClusteringAnalysis()
    success = analyzer.run_complete_clustering_analysis()
    
    if success:
        print("\nüéâ ANALYSE DE CLUSTERING R√âUSSIE!")
        print("\nüí° INT√âGRATION DANS VOTRE M√âMOIRE:")
        print("‚Ä¢ Chapitre 3: M√©thodologie - Justification du choix d'algorithme")
        print("‚Ä¢ Chapitre 4: R√©sultats - Classification des r√©gimes climatiques")
        print("‚Ä¢ Chapitre 5: Discussion - Patterns identifi√©s par clustering")
        print("‚Ä¢ Annexes: Comparaison d√©taill√©e des algorithmes")
        print("\nConsultez le dossier outputs/visualizations/clustering/")
        return 0
    else:
        print("\n‚ùå √âCHEC DE L'ANALYSE DE CLUSTERING")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
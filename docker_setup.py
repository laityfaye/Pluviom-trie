#!/usr/bin/env python3
"""
Script d'adaptation Docker pour projet existant
Adapte la structure existante pour la containerisation Docker

Usage:
    python docker_setup.py

Ce script va:
1. Analyser votre structure existante
2. CrÃ©er la structure Docker nÃ©cessaire
3. GÃ©nÃ©rer les Dockerfiles adaptÃ©s
4. CrÃ©er docker-compose.yml
5. Configurer les variables d'environnement
"""

import os
import sys
from pathlib import Path
import shutil
import json

class DockerSetupManager:
    def __init__(self, project_root=None):
        self.project_root = Path(project_root) if project_root else Path.cwd()
        self.docker_structure = {
            'docker': ['api', 'ml-pipeline', 'dashboard', 'timescaledb'],
            'services': ['api', 'ml-pipeline', 'dashboard', 'database'],
            'config': ['docker']
        }
        
    def analyze_existing_structure(self):
        """Analyse la structure existante du projet."""
        print("ğŸ” Analyse de la structure existante...")
        print("=" * 60)
        
        existing_structure = {
            'files': [],
            'directories': [],
            'python_files': [],
            'key_components': {}
        }
        
        # Parcourir le projet existant
        for item in self.project_root.iterdir():
            if item.is_file():
                existing_structure['files'].append(item.name)
                if item.suffix == '.py':
                    existing_structure['python_files'].append(item.name)
            elif item.is_dir() and not item.name.startswith('.'):
                existing_structure['directories'].append(item.name)
        
        # Identifier les composants clÃ©s
        key_files = {
            'main.py': 'Point d\'entrÃ©e principal',
            'requirements.txt': 'DÃ©pendances Python',
            'scripts/': 'Scripts d\'analyse ML',
            'src/': 'Code source modulaire',
            'data/': 'DonnÃ©es du projet',
            'outputs/': 'RÃ©sultats et visualisations'
        }
        
        for key_file, description in key_files.items():
            if '/' in key_file:
                # Dossier
                if (self.project_root / key_file.rstrip('/')).exists():
                    existing_structure['key_components'][key_file] = {
                        'type': 'directory',
                        'description': description,
                        'exists': True
                    }
            else:
                # Fichier
                if (self.project_root / key_file).exists():
                    existing_structure['key_components'][key_file] = {
                        'type': 'file',
                        'description': description,
                        'exists': True
                    }
        
        # Affichage du rÃ©sumÃ©
        print("ğŸ“ Structure existante dÃ©tectÃ©e:")
        for component, info in existing_structure['key_components'].items():
            status = "âœ…" if info['exists'] else "âŒ"
            print(f"  {status} {component:<20} - {info['description']}")
        
        print(f"\nğŸ“Š Statistiques:")
        print(f"  Fichiers Python: {len(existing_structure['python_files'])}")
        print(f"  Dossiers: {len(existing_structure['directories'])}")
        
        return existing_structure
    
    def create_docker_structure(self):
        """CrÃ©e la structure Docker nÃ©cessaire."""
        print("\nğŸ—ï¸  CrÃ©ation de la structure Docker...")
        print("=" * 60)
        
        # CrÃ©er les dossiers Docker
        docker_dirs = [
            'docker/api',
            'docker/ml-pipeline', 
            'docker/dashboard',
            'docker/timescaledb/init-scripts',
            'services/api',
            'services/ml-pipeline',
            'services/dashboard',
            'services/database',
            'config/docker',
            'monitoring'
        ]
        
        for dir_path in docker_dirs:
            full_path = self.project_root / dir_path
            full_path.mkdir(parents=True, exist_ok=True)
            print(f"  ğŸ“‚ CrÃ©Ã©: {dir_path}")
        
        return True
    
    def generate_dockerfile_ml_pipeline(self):
        """GÃ©nÃ¨re le Dockerfile pour le pipeline ML."""
        dockerfile_content = """# Dockerfile pour Pipeline ML - Projet Climat SÃ©nÃ©gal
FROM python:3.9-slim

# Variables d'environnement
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV DEBIAN_FRONTEND=noninteractive

# MÃ©tadonnÃ©es
LABEL maintainer="Laity FAYE"
LABEL description="Pipeline ML pour prÃ©diction Ã©vÃ©nements climatiques extrÃªmes"
LABEL version="1.0.0"

# DÃ©pendances systÃ¨me pour scientific computing
RUN apt-get update && apt-get install -y \\
    gcc \\
    g++ \\
    gfortran \\
    libopenblas-dev \\
    liblapack-dev \\
    libhdf5-dev \\
    pkg-config \\
    curl \\
    && rm -rf /var/lib/apt/lists/*

# RÃ©pertoire de travail
WORKDIR /app

# Copie et installation des dÃ©pendances
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip
RUN pip install --no-cache-dir -r requirements.txt

# Copie de la structure du projet
COPY src/ ./src/
COPY scripts/ ./scripts/
COPY main.py .
COPY quick_test.py .

# CrÃ©ation des dossiers de sortie
RUN mkdir -p data/raw data/processed outputs/models outputs/reports outputs/visualizations

# Permissions d'exÃ©cution
RUN chmod +x main.py

# Variables d'environnement pour le projet
ENV PYTHONPATH=/app:/app/src
ENV PROJECT_ROOT=/app
ENV DATA_PATH=/app/data
ENV OUTPUT_PATH=/app/outputs

# Healthcheck
HEALTHCHECK --interval=60s --timeout=30s --start-period=10s --retries=3 \\
  CMD python quick_test.py || exit 1

# Point d'entrÃ©e par dÃ©faut
CMD ["python", "main.py", "--only-ml"]
"""
        
        dockerfile_path = self.project_root / 'docker/ml-pipeline/Dockerfile'
        with open(dockerfile_path, 'w', encoding='utf-8') as f:
            f.write(dockerfile_content)
        
        print("  ğŸ“„ Dockerfile ML Pipeline crÃ©Ã©")
        return dockerfile_path
    
    def generate_dockerfile_api(self):
        """GÃ©nÃ¨re le Dockerfile pour l'API FastAPI."""
        dockerfile_content = """# Dockerfile pour API - Projet Climat SÃ©nÃ©gal
FROM python:3.9-slim

ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

# DÃ©pendances systÃ¨me lÃ©gÃ¨res
RUN apt-get update && apt-get install -y \\
    gcc \\
    curl \\
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Installation des dÃ©pendances API
COPY services/api/requirements.txt .
RUN pip install --no-cache-dir --upgrade pip
RUN pip install --no-cache-dir -r requirements.txt

# Copie du code API
COPY services/api/ .

# Copie des modÃ¨les ML prÃ©-entraÃ®nÃ©s
COPY outputs/models/ ./models/

# Port d'exposition
EXPOSE 8000

# Healthcheck
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\
  CMD curl -f http://localhost:8000/health || exit 1

# Variables d'environnement
ENV API_HOST=0.0.0.0
ENV API_PORT=8000
ENV MODEL_PATH=/app/models

# Commande de dÃ©marrage
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
"""
        
        dockerfile_path = self.project_root / 'docker/api/Dockerfile'
        with open(dockerfile_path, 'w', encoding='utf-8') as f:
            f.write(dockerfile_content)
        
        print("  ğŸ“„ Dockerfile API crÃ©Ã©")
        return dockerfile_path
    
    def generate_api_service_files(self):
        """GÃ©nÃ¨re les fichiers de base pour le service API."""
        
        # Requirements API
        api_requirements = """fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
sqlalchemy==2.0.23
psycopg2-binary==2.9.9
pandas==2.1.3
numpy==1.24.3
scikit-learn==1.3.2
joblib==1.3.2
python-multipart==0.0.6
python-dotenv==1.0.0
prometheus-client==0.19.0
"""
        
        api_req_path = self.project_root / 'services/api/requirements.txt'
        with open(api_req_path, 'w', encoding='utf-8') as f:
            f.write(api_requirements)
        
        # API main.py
        api_main_content = '''"""
API FastAPI pour le projet de prÃ©diction climatique
"""
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import pandas as pd
import numpy as np
import joblib
from pathlib import Path
import os
from datetime import datetime, timedelta
import logging

# Configuration logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Climate Extreme Prediction API",
    description="API pour prÃ©dictions d'Ã©vÃ©nements climatiques extrÃªmes au SÃ©nÃ©gal",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configuration des chemins
MODEL_PATH = Path(os.getenv("MODEL_PATH", "./models"))

# ModÃ¨les chargÃ©s en mÃ©moire
loaded_models = {}

class PredictionRequest(BaseModel):
    features: dict
    model_name: str = "random_forest"

class PredictionResponse(BaseModel):
    prediction: float
    probability: float
    confidence_interval: dict
    model_used: str
    timestamp: datetime

@app.on_event("startup")
async def startup_event():
    """Initialisation de l'API."""
    logger.info("ğŸš€ DÃ©marrage de l'API Climate Prediction")
    
    # Charger les modÃ¨les disponibles
    if MODEL_PATH.exists():
        for model_file in MODEL_PATH.glob("*.pkl"):
            try:
                model_name = model_file.stem
                loaded_models[model_name] = joblib.load(model_file)
                logger.info(f"âœ… ModÃ¨le chargÃ©: {model_name}")
            except Exception as e:
                logger.error(f"âŒ Erreur chargement {model_file}: {e}")
    
    logger.info(f"ğŸ“Š {len(loaded_models)} modÃ¨les disponibles")

@app.get("/")
async def root():
    return {
        "message": "API Climate Extreme Prediction",
        "version": "1.0.0",
        "status": "running",
        "models_available": list(loaded_models.keys())
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now(),
        "models_loaded": len(loaded_models)
    }

@app.get("/models")
async def list_models():
    return {
        "available_models": list(loaded_models.keys()),
        "total_models": len(loaded_models)
    }

@app.post("/predict", response_model=PredictionResponse)
async def predict_extreme_event(request: PredictionRequest):
    """PrÃ©dire la probabilitÃ© d'un Ã©vÃ©nement extrÃªme."""
    
    if request.model_name not in loaded_models:
        raise HTTPException(
            status_code=404,
            detail=f"ModÃ¨le '{request.model_name}' non trouvÃ©. ModÃ¨les disponibles: {list(loaded_models.keys())}"
        )
    
    try:
        model = loaded_models[request.model_name]
        
        # Convertir les features en format appropriÃ©
        features_df = pd.DataFrame([request.features])
        
        # PrÃ©diction
        if hasattr(model, 'predict_proba'):
            proba = model.predict_proba(features_df)[0]
            prediction = model.predict(features_df)[0]
            probability = float(proba[1]) if len(proba) > 1 else float(proba[0])
        else:
            prediction = model.predict(features_df)[0]
            probability = float(prediction)
        
        # Intervalle de confiance simulÃ© (Ã  amÃ©liorer avec vrais modÃ¨les)
        confidence_interval = {
            "lower": max(0, probability - 0.1),
            "upper": min(1, probability + 0.1)
        }
        
        return PredictionResponse(
            prediction=float(prediction),
            probability=probability,
            confidence_interval=confidence_interval,
            model_used=request.model_name,
            timestamp=datetime.now()
        )
        
    except Exception as e:
        logger.error(f"Erreur prÃ©diction: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/metrics")
async def get_metrics():
    """MÃ©triques Prometheus."""
    # Ã€ implÃ©menter avec prometheus_client
    return {"message": "MÃ©triques Ã  implÃ©menter"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
'''
        
        api_main_path = self.project_root / 'services/api/main.py'
        with open(api_main_path, 'w', encoding='utf-8') as f:
            f.write(api_main_content)
        
        print("  ğŸ“„ Service API crÃ©Ã©")
    
    def generate_docker_compose(self):
        """GÃ©nÃ¨re le fichier docker-compose.yml principal."""
        compose_content = """version: '3.8'

services:
  # Base de donnÃ©es TimescaleDB
  timescaledb:
    image: timescale/timescaledb:latest-pg14
    container_name: climate-timescaledb
    environment:
      - POSTGRES_DB=${POSTGRES_DB:-climate_db}
      - POSTGRES_USER=${POSTGRES_USER:-climate_user}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-secure_password}
      - PGDATA=/var/lib/postgresql/data/pgdata
    volumes:
      - timescale_data:/var/lib/postgresql/data
      - ./docker/timescaledb/init-scripts:/docker-entrypoint-initdb.d
      - ./data/backup:/backup
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER:-climate_user} -d $${POSTGRES_DB:-climate_db}"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - climate-network

  # Redis pour cache et files d'attente
  redis:
    image: redis:7-alpine
    container_name: climate-redis
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - climate-network

  # Pipeline ML - Traitement existant
  ml-pipeline:
    build: 
      context: .
      dockerfile: docker/ml-pipeline/Dockerfile
    container_name: climate-ml-pipeline
    environment:
      - DATABASE_URL=postgresql://$${POSTGRES_USER:-climate_user}:$${POSTGRES_PASSWORD:-secure_password}@timescaledb:5432/$${POSTGRES_DB:-climate_db}
      - REDIS_URL=redis://redis:6379
      - PYTHONPATH=/app:/app/src
    depends_on:
      timescaledb:
        condition: service_healthy
    volumes:
      - ./data:/app/data
      - ./outputs:/app/outputs
      - ml_models:/app/outputs/models
    working_dir: /app
    networks:
      - climate-network
    # Note: restart="no" pour exÃ©cution manuelle du pipeline
    restart: "no"

  # API FastAPI
  api:
    build:
      context: .
      dockerfile: docker/api/Dockerfile
    container_name: climate-api
    environment:
      - DATABASE_URL=postgresql://$${POSTGRES_USER:-climate_user}:$${POSTGRES_PASSWORD:-secure_password}@timescaledb:5432/$${POSTGRES_DB:-climate_db}
      - REDIS_URL=redis://redis:6379
      - MODEL_PATH=/app/models
    depends_on:
      timescaledb:
        condition: service_healthy
      redis:
        condition: service_healthy
    ports:
      - "8000:8000"
    volumes:
      - ml_models:/app/models:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - climate-network

  # Dashboard Web (Ã  dÃ©velopper)
  dashboard:
    image: nginx:alpine
    container_name: climate-dashboard
    ports:
      - "3000:80"
    volumes:
      - ./services/dashboard/dist:/usr/share/nginx/html:ro
      - ./docker/dashboard/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - api
    restart: unless-stopped
    networks:
      - climate-network

  # Monitoring - Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: climate-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    networks:
      - climate-network

  # Monitoring - Grafana
  grafana:
    image: grafana/grafana:latest
    container_name: climate-grafana
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    depends_on:
      - prometheus
    restart: unless-stopped
    networks:
      - climate-network

volumes:
  timescale_data:
    driver: local
  redis_data:
    driver: local
  ml_models:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

networks:
  climate-network:
    driver: bridge
    name: senegal-climate-network
"""
        
        compose_path = self.project_root / 'docker-compose.yml'
        with open(compose_path, 'w', encoding='utf-8') as f:
            f.write(compose_content)
        
        print("  ğŸ“„ docker-compose.yml crÃ©Ã©")
        return compose_path
    
    def generate_env_file(self):
        """GÃ©nÃ¨re le fichier .env avec les variables d'environnement."""
        env_content = """# Configuration Docker - Projet Climat SÃ©nÃ©gal
# Base de donnÃ©es TimescaleDB
POSTGRES_DB=climate_db
POSTGRES_USER=climate_user
POSTGRES_PASSWORD=ChangeMe_SecurePassword2024!

# API Configuration
API_SECRET_KEY=your_super_secure_api_key_here_change_me
API_DEBUG=false
API_HOST=0.0.0.0
API_PORT=8000

# Monitoring
GRAFANA_PASSWORD=secure_grafana_password_2024

# Pipeline ML
ML_MODEL_PATH=/app/outputs/models
DATA_PATH=/app/data
OUTPUT_PATH=/app/outputs

# Redis Configuration
REDIS_URL=redis://redis:6379

# Projet Configuration
COMPOSE_PROJECT_NAME=senegal-climate
PROJECT_NAME=senegal-extreme-climate-prediction

# Environnement
ENVIRONMENT=development
DEBUG=true

# Logging
LOG_LEVEL=INFO
"""
        
        env_path = self.project_root / '.env'
        if not env_path.exists():  # Ne pas Ã©craser si existe dÃ©jÃ 
            with open(env_path, 'w', encoding='utf-8') as f:
                f.write(env_content)
            print("  ğŸ“„ .env crÃ©Ã©")
        else:
            print("  ğŸ“„ .env existe dÃ©jÃ  (prÃ©servÃ©)")
    
    def generate_dockerignore(self):
        """GÃ©nÃ¨re le fichier .dockerignore."""
        dockerignore_content = """# Git
.git
.gitignore
*.md

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
venv/
env/
ENV/
env.bak/
venv.bak/

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
Thumbs.db

# Logs
*.log
logs/

# Cache
.cache/
.pytest_cache/
.coverage

# Temporary files
*.tmp
*.temp

# Large data files (Ã  adapter selon votre projet)
data/raw/*.nc
data/raw/*.mat
*.hdf5

# Build artifacts
docker-compose.override.yml
.env.local
.env.*.local

# Documentation
docs/_build/

# Jupyter
.jupyter/
*.ipynb_checkpoints
"""
        
        dockerignore_path = self.project_root / '.dockerignore'
        with open(dockerignore_path, 'w', encoding='utf-8') as f:
            f.write(dockerignore_content)
        
        print("  ğŸ“„ .dockerignore crÃ©Ã©")
    
    def generate_monitoring_config(self):
        """GÃ©nÃ¨re les configurations de monitoring."""
        
        # Prometheus config
        prometheus_config = """global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  # - "first_rules.yml"
  # - "second_rules.yml"

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
  
  - job_name: 'climate-api'
    static_configs:
      - targets: ['api:8000']
    metrics_path: '/metrics'
    scrape_interval: 30s
  
  - job_name: 'timescaledb'
    static_configs:
      - targets: ['timescaledb:5432']
"""
        
        prometheus_dir = self.project_root / 'monitoring'
        prometheus_dir.mkdir(exist_ok=True)
        
        with open(prometheus_dir / 'prometheus.yml', 'w', encoding='utf-8') as f:
            f.write(prometheus_config)
        
        print("  ğŸ“„ Configuration monitoring crÃ©Ã©e")
    
    def generate_scripts(self):
        """GÃ©nÃ¨re les scripts utilitaires Docker."""
        
        # CrÃ©er le dossier docker/scripts pour Ã©viter confusion
        docker_scripts_dir = self.project_root / 'docker/scripts'
        docker_scripts_dir.mkdir(exist_ok=True)
        
        # Script de dÃ©marrage PowerShell
        startup_script = """# startup.ps1 - Script de dÃ©marrage du projet Docker
param(
    [string]$Action = "start",
    [switch]$Build = $false,
    [switch]$Reset = $false
)

Write-Host "ğŸ³ Gestion Docker - Projet Climat SÃ©nÃ©gal" -ForegroundColor Green
Write-Host "=" * 50

switch ($Action.ToLower()) {
    "start" {
        Write-Host "â–¶ï¸ DÃ©marrage des services..." -ForegroundColor Yellow
        if ($Build) {
            docker-compose up -d --build
        } else {
            docker-compose up -d
        }
        
        Write-Host "ğŸ¥ VÃ©rification de la santÃ©..." -ForegroundColor Yellow
        Start-Sleep -Seconds 10
        docker-compose ps
        
        Write-Host "ğŸŒ Services disponibles:" -ForegroundColor Cyan
        Write-Host "  API: http://localhost:8000" -ForegroundColor White
        Write-Host "  Dashboard: http://localhost:3000" -ForegroundColor White
        Write-Host "  Grafana: http://localhost:3001" -ForegroundColor White
        Write-Host "  Prometheus: http://localhost:9090" -ForegroundColor White
    }
    
    "stop" {
        Write-Host "â¹ï¸ ArrÃªt des services..." -ForegroundColor Yellow
        docker-compose down
    }
    
    "logs" {
        Write-Host "ğŸ“„ Logs des services..." -ForegroundColor Yellow
        docker-compose logs -f
    }
    
    "ml" {
        Write-Host "ğŸ¤– ExÃ©cution du pipeline ML..." -ForegroundColor Yellow
        docker-compose run --rm ml-pipeline python main.py --only-ml
    }
    
    "reset" {
        Write-Host "ğŸ”„ RÃ©initialisation complÃ¨te..." -ForegroundColor Red
        docker-compose down -v
        docker system prune -f
        docker-compose up -d --build
    }
    
    default {
        Write-Host "Usage: ./startup.ps1 [start|stop|logs|ml|reset] [-Build] [-Reset]" -ForegroundColor Yellow
    }
}
"""
        
        # Sauvegarder dans docker/scripts/
        startup_script_path = docker_scripts_dir / 'startup.ps1'
        with open(startup_script_path, 'w', encoding='utf-8') as f:
            f.write(startup_script)
        
        # CrÃ©er aussi un lien dans la racine pour facilitÃ© d'usage
        root_startup_path = self.project_root / 'docker-start.ps1'
        with open(root_startup_path, 'w', encoding='utf-8') as f:
            f.write(startup_script)
        
        # Script de dÃ©ploiement
        deploy_script = """# deploy.ps1 - Script de dÃ©ploiement Docker
param(
    [string]$Environment = "dev",
    [switch]$Build = $false,
    [switch]$Migrate = $false,
    [switch]$Reset = $false
)

Write-Host "ğŸš€ DÃ©ploiement Docker - Environnement: $Environment" -ForegroundColor Green
Write-Host "=" * 60

if ($Reset) {
    Write-Host "ğŸ”„ RÃ©initialisation complÃ¨te..." -ForegroundColor Red
    docker-compose down -v
    docker system prune -f
    $Build = $true
}

if ($Build) {
    Write-Host "ğŸ”¨ Construction des images Docker..." -ForegroundColor Yellow
    docker-compose build --no-cache
}

if ($Migrate) {
    Write-Host "ğŸ“Š Migration base de donnÃ©es..." -ForegroundColor Yellow
    docker-compose run --rm ml-pipeline python -c "print('Migration simulÃ©e - Ã  implÃ©menter')"
}

Write-Host "â–¶ï¸ DÃ©marrage des services..." -ForegroundColor Yellow
docker-compose up -d

Write-Host "ğŸ¥ VÃ©rification de la santÃ© des services..." -ForegroundColor Yellow
Start-Sleep -Seconds 30

$services = @("timescaledb", "redis", "api")
foreach ($service in $services) {
    $status = docker-compose ps -q $service
    if ($status) {
        $health = docker inspect $status --format='{{.State.Status}}'
        if ($health -eq "running") {
            Write-Host "âœ… $service: Running" -ForegroundColor Green
        } else {
            Write-Host "âŒ $service: $health" -ForegroundColor Red
        }
    } else {
        Write-Host "âŒ $service: Not found" -ForegroundColor Red
    }
}

Write-Host "ğŸ‰ DÃ©ploiement terminÃ©!" -ForegroundColor Green
Write-Host "ğŸŒ Services disponibles:" -ForegroundColor Cyan
Write-Host "  Dashboard: http://localhost:3000" -ForegroundColor White
Write-Host "  API: http://localhost:8000" -ForegroundColor White
Write-Host "  Grafana: http://localhost:3001" -ForegroundColor White
"""
        
        deploy_script_path = docker_scripts_dir / 'deploy.ps1'
        with open(deploy_script_path, 'w', encoding='utf-8') as f:
            f.write(deploy_script)
        
        # Script de sauvegarde
        backup_script = """# backup.ps1 - Script de sauvegarde Docker
param(
    [string]$BackupName = (Get-Date -Format "yyyy-MM-dd_HH-mm-ss")
)

Write-Host "ğŸ’¾ Sauvegarde Docker - $BackupName" -ForegroundColor Green

$backupDir = "data/backup"
if (-not (Test-Path $backupDir)) {
    New-Item -ItemType Directory -Path $backupDir -Force
}

Write-Host "ğŸ“Š Sauvegarde base de donnÃ©es..." -ForegroundColor Yellow
docker-compose exec -T timescaledb pg_dump -U climate_user climate_db > "$backupDir/db_$BackupName.sql"

Write-Host "ğŸ¤– Sauvegarde modÃ¨les ML..." -ForegroundColor Yellow
docker cp climate-ml-pipeline:/app/outputs/models "$backupDir/models_$BackupName"

Write-Host "âœ… Sauvegarde terminÃ©e: $backupDir" -ForegroundColor Green
"""
        
        backup_script_path = docker_scripts_dir / 'backup.ps1'
        with open(backup_script_path, 'w', encoding='utf-8') as f:
            f.write(backup_script)
        
        print("  ğŸ“„ Scripts Docker crÃ©Ã©s dans docker/scripts/")
        print("  ğŸ“„ Raccourci docker-start.ps1 crÃ©Ã© Ã  la racine")
    
    def run_setup(self):
        """ExÃ©cute le setup complet."""
        print("ğŸ³ SETUP DOCKER - ADAPTATION STRUCTURE EXISTANTE")
        print("=" * 80)
        print(f"ğŸ“ Projet: {self.project_root}")
        print("=" * 80)
        
        # Ã‰tape 1: Analyser l'existant
        existing = self.analyze_existing_structure()
        
        # Ã‰tape 2: CrÃ©er structure Docker
        self.create_docker_structure()
        
        # Ã‰tape 3: GÃ©nÃ©rer Dockerfiles
        print("\nğŸ”¨ GÃ©nÃ©ration des Dockerfiles...")
        self.generate_dockerfile_ml_pipeline()
        self.generate_dockerfile_api()
        
        # Ã‰tape 4: GÃ©nÃ©rer services
        print("\nâš™ï¸ CrÃ©ation des services...")
        self.generate_api_service_files()
        
        # Ã‰tape 5: Orchestration
        print("\nğŸ¼ Configuration Docker Compose...")
        self.generate_docker_compose()
        self.generate_env_file()
        self.generate_dockerignore()
        
        # Ã‰tape 6: Monitoring
        print("\nğŸ“Š Configuration monitoring...")
        self.generate_monitoring_config()
        
        # Ã‰tape 7: Scripts utilitaires
        print("\nğŸ› ï¸ GÃ©nÃ©ration des scripts...")
        self.generate_scripts()
        
        # RÃ©sumÃ© final
        print("\n" + "=" * 80)
        print("âœ… SETUP DOCKER TERMINÃ‰!")
        print("=" * 80)
        print("ğŸ“‚ Structure crÃ©Ã©e:")
        print("  â”œâ”€â”€ docker/")
        print("  â”‚   â”œâ”€â”€ api/Dockerfile")
        print("  â”‚   â”œâ”€â”€ ml-pipeline/Dockerfile")
        print("  â”‚   â””â”€â”€ timescaledb/")
        print("  â”œâ”€â”€ services/")
        print("  â”‚   â””â”€â”€ api/")
        print("  â”œâ”€â”€ docker-compose.yml")
        print("  â”œâ”€â”€ .env")
        print("  â””â”€â”€ startup.ps1")
        
        print("\nğŸš€ Prochaines Ã©tapes:")
        print("  1. VÃ©rifiez et modifiez .env selon vos besoins")
        print("  2. Lancez: docker-compose up -d --build")
        print("  3. Testez le pipeline: docker-compose run --rm ml-pipeline python main.py")
        print("  4. AccÃ©dez Ã  l'API: http://localhost:8000")
        
        return True

def main():
    """Point d'entrÃ©e principal."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Setup Docker pour projet existant")
    parser.add_argument("--project-root", help="Chemin racine du projet", default=".")
    
    args = parser.parse_args()
    
    setup_manager = DockerSetupManager(args.project_root)
    
    try:
        setup_manager.run_setup()
        return 0
    except Exception as e:
        print(f"\nâŒ Erreur lors du setup: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())